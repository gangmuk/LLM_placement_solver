parameter,value
sequence_length,2048
batch_size,16
model_name,llama-70b
num_decoder_layers,80
d_model,8192
d_hidden,28672
vocab_size,32000
num_attention_heads,64
layer_weight_memory_gb,0.5
time_limit_seconds,300
optimality_gap,0.01