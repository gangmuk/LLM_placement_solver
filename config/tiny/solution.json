{
  "config": {
    "model_name": "llama-7b",
    "num_decoder_layers": 4,
    "sequence_length": 2048,
    "batch_size": 16,
    "d_model": 4096,
    "d_hidden": 11008
  },
  "solution": {
    "objective_value": 336.6,
    "gpu_assignments": [
      {
        "gpu_type": "A100",
        "gpu_id": 1,
        "global_gpu_id": 1,
        "start_layer": 1,
        "end_layer": 4,
        "segment_size": 4,
        "throughput": 336.6
      }
    ],
    "network_connections": [],
    "solve_status": 2
  }
}