parameter,value
sequence_length,4096
batch_size,32
model_name,llama-70b
num_decoder_layers,80
d_model,8192
d_hidden,22016
vocab_size,32000
num_attention_heads,64
layer_weight_memory_gb,3.5
time_limit_seconds,120
optimality_gap,0.05