parameter,value
sequence_length,2048
batch_size,16
model_name,llama-13b
num_decoder_layers,20
d_model,4096
d_hidden,11008
vocab_size,32000
num_attention_heads,32
layer_weight_memory_gb,2.0
time_limit_seconds,120
optimality_gap,0.05
