parameter,value
sequence_length,4096
min_batch_size,16
max_batch_size,64
model_name,llama-70b
num_decoder_layers,40
d_model,8192
d_hidden,22016
vocab_size,32000
num_attention_heads,64
layer_weight_memory_gb,3.5
time_limit_seconds,300
optimality_gap,0.05
bytes_per_element,2
enable_segment_quantization,true
max_pipeline_stages,8
min_layers_per_stage,5
cost_throughput_weight,0.95
max_hourly_cost,999.0
max_cost_per_token,0.002
throughput_normalization,10000.0
cost_normalization,1.0